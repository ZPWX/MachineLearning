__决策树是一个类似于流程图的树结构。其中，每个内部<br>
节点表示在一个属性上的测试，每个分支代表一个属性输出，<br>
而每个树节点代表类或者类分布。树的最顶层是根节点。__

#### 1. 构造决策树
  > 1. 信息增益/信息增益比。（特征选择）---ID3/C4.5算法
  > 2. 决策树的生成。 （根结点选择）
  > 3. 决策树的剪枝/拟合状态。 （防止过拟合）
  ##### 1.1. 信息增益
    * 输入： 训练集D和特征值A
    * 输出： 特征值A对训练数据集D的信息增益 g(D,A)
    
 #### 2. 熵
  > 1. 计算数据集D的经验熵H(D):
  <br>   H(D) = -
  > 2. 计算特征值A对数据集D的经验条件熵H(D|A):
  > 3. 计算信息增益:
  <br>   g(D,A) = H(D) - H(D|A)
  ##### 2.1 机器学习中分类和预测算法的评估：
    * 准确性
    * 速度
    * 强壮性
    * 可规模性
    * 可解释性
    
  ##### 2.2 信息熵
  一条信息的信息量大小和它的不确定性有直接的关系，要搞清楚意见非常<br>
  不确定的事情，或者是我们一无所知的事情，需要了解大量的信<br>息。而`信息量的度量就等于不确定性的多少`。<br>
  __变量的不确定性越大，熵也就越大.__
  
 #### 3. 决策树归纳法 -ID3
    选择属性判断节点
  ##### 3.1 C4.5
  * 共同点：都是贪心算法，自上而下进行
  * 区别： 属性选择度量方法不同。C4.5是CART
 #### 4. 剪枝
  `防止过拟合情况发生(overfitting)`
  * 先剪枝
  * 后剪枝
 #### 5. 决策树优缺点
 * 优点： 直观，便于理解，小规模数据集有效
 * 缺点： 处理连续变量不好；类别较多时，错误增加的比较快；可规模性一般；
